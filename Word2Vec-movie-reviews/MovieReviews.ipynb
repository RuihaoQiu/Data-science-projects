{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec for movie reviews\n",
    "\n",
    "Build word2vec model using tensorflow, with movie review data (5331 positive and 5331 negative snippets) from http://www.cs.cornell.edu/people/pabo/movie-review-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simplistic , silly and tedious . \\n',\n",
       " \"it's so laddish and juvenile , only teenage boys could possibly find it funny . \\n\",\n",
       " 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \\n',\n",
       " '[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \\n',\n",
       " 'a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \\n',\n",
       " \"the story is also as unoriginal as they come , already having been recycled more times than i'd care to count . \\n\",\n",
       " \"about the only thing to give the movie points for is bravado -- to take an entirely stale concept and push it through the audience's meat grinder one more time . \\n\",\n",
       " 'not so much farcical as sour . \\n',\n",
       " 'unfortunately the story and the actors are served with a hack script . \\n',\n",
       " 'all the more disquieting for its relatively gore-free allusions to the serial murders , but it falls down in its attempts to humanize its subject . \\n']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import random\n",
    "\n",
    "file_pos = open(\"rt-polarity-pos.txt\", \"r\", encoding=\"ISO-8859-1\")\n",
    "data_pos = file_pos.readlines()\n",
    "data_pos[:10]\n",
    "file_neg = open(\"rt-polarity-neg.txt\", \"r\", encoding=\"ISO-8859-1\")\n",
    "data_neg = file_neg.readlines()\n",
    "data_neg[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious . \\n</td>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>effective but too-tepid biopic\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the story is also as unoriginal as they come ,...</td>\n",
       "      <td>the film provides some great insight into the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>about the only thing to give the movie points ...</td>\n",
       "      <td>offers that rare combination of entertainment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not so much farcical as sour . \\n</td>\n",
       "      <td>perhaps no picture ever made has more literall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unfortunately the story and the actors are ser...</td>\n",
       "      <td>steers turns in a snappy screenplay that curls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>all the more disquieting for its relatively go...</td>\n",
       "      <td>take care of my cat offers a refreshingly diff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                simplistic , silly and tedious . \\n   \n",
       "1  it's so laddish and juvenile , only teenage bo...   \n",
       "2  exploitative and largely devoid of the depth o...   \n",
       "3  [garbus] discards the potential for pathologic...   \n",
       "4  a visually flashy but narratively opaque and e...   \n",
       "5  the story is also as unoriginal as they come ,...   \n",
       "6  about the only thing to give the movie points ...   \n",
       "7                  not so much farcical as sour . \\n   \n",
       "8  unfortunately the story and the actors are ser...   \n",
       "9  all the more disquieting for its relatively go...   \n",
       "\n",
       "                                                   1  \n",
       "0  the rock is destined to be the 21st century's ...  \n",
       "1  the gorgeously elaborate continuation of \" the...  \n",
       "2                   effective but too-tepid biopic\\n  \n",
       "3  if you sometimes like to go to the movies to h...  \n",
       "4  emerges as something rare , an issue movie tha...  \n",
       "5  the film provides some great insight into the ...  \n",
       "6  offers that rare combination of entertainment ...  \n",
       "7  perhaps no picture ever made has more literall...  \n",
       "8  steers turns in a snappy screenplay that curls...  \n",
       "9  take care of my cat offers a refreshingly diff...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([data_neg, data_pos]).T\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['simplistic', 'silly', 'and', 'tedious'], ['it', 'so', 'laddish', 'and', 'juvenile', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny'], ['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable'], ['garbus', 'discards', 'the', 'potential', 'for', 'pathological', 'study', 'exhuming', 'instead', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation'], ['a', 'visually', 'flashy', 'but', 'narratively', 'opaque', 'and', 'emotionally', 'vapid', 'exercise', 'in', 'style', 'and', 'mystification']]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "\n",
    "def clean_sentence(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()&,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"'s\", '')\n",
    "    text = text.replace(\"/\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    text=re.sub( '\\s+', ' ', text).strip()\n",
    "    \n",
    "    words = text.split(\" \")\n",
    "    for word in list(words):\n",
    "    #    if word in STOP_WORDS:\n",
    "    #        words.remove(word)\n",
    "        if word == \"\":\n",
    "            words.remove(word)\n",
    "    return words\n",
    "\n",
    "corpus = []\n",
    "for sentence in df[:][0]:\n",
    "    #print(clean_sentence(sentence))\n",
    "    #corpus += clean_sentence(sentence)\n",
    "    corpus.append(clean_sentence(sentence))\n",
    "    \n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sum(corpus[:100], [])\n",
    "\n",
    "vocabulary_size = 1000\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(sample, vocabulary_size)\n",
    "\n",
    "#print('Most common words (+UNK)', count[:5])\n",
    "#print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "simplistic\n",
      "5 and -> 699 silly\n",
      "silly\n",
      "5 and -> 154 simplistic\n",
      "and\n",
      "5 and -> 112 tedious\n",
      "tedious\n",
      "5 and -> 9 it\n",
      "it\n",
      "112 tedious -> 699 silly\n",
      "so\n",
      "112 tedious -> 5 and\n",
      "laddish\n",
      "112 tedious -> 9 it\n",
      "and\n",
      "112 tedious -> 25 so\n",
      "juvenile\n",
      "9 it -> 112 tedious\n",
      "only\n",
      "9 it -> 25 so\n",
      "teenage\n",
      "9 it -> 5 and\n",
      "boys\n",
      "9 it -> 480 laddish\n",
      "could\n",
      "25 so -> 5 and\n",
      "possibly\n",
      "25 so -> 480 laddish\n",
      "find\n",
      "25 so -> 112 tedious\n",
      "it\n",
      "25 so -> 9 it\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    data_index = 0\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    print(batch_size // num_skips)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=16, num_skips=4, skip_window=2)\n",
    "\n",
    "\n",
    "for i in range(16):\n",
    "    print(reverse_dictionary[data[i]])\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
