{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on movie reviews\n",
    "\n",
    "This NLP project is aim to a classifiers for movie review scraped from IMDb.com:\n",
    "\n",
    "1. Crawl data from IMDb.com, the top-100 most popular movies by genre (total 20 genres - Action, Adventure ...), refer to the website - https://www.imdb.com/chart/moviemeter?ref_=nv_mv_mpm_8 . The code of the spider is in the document \"Movie-crawler\".\n",
    "2. Sentiment Analysis, build a classifier to predict the rating of a review, score from 1-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12999, 4)\n",
      "[ \"Your going to be hard pressed to find a movie more over the top than Deadpool 2, David leitch takes it to a whole new level entirely. It works namely because the violence here is just to creative. Sure it defies all Logic most of the time, but i was laughing and having a rip-roaring time throughout.,Go into this movie knowing that it is a campy, corny, over the top superhero/action/comedy that is just about crazy shooting sequences and one-liners, and you'll be fine. Do not go into this movie expecting deep plot, meaningful conversations among characters, or anything remotely resembling a serious action or drama movie. ,Loved it....\"\n",
      " \"I was able to see Deadpool 2 at an advance screening. Deadpool shook things up and delivered offbeat fun when diehard fans were looking for something new from the genre. I thought it was a surprisingly well-rounded movie that perfectly encapsulated its hero and lovingly poked fun at both itself and other genre tropes. It was among my favourite movies of 2016. When it comes to bringing the laughs, I think Deadpool 2 is just as funny as the original entry. There's even more meta jokes, the same gleefully dirty spirit carries over and some unexpected payoffs left me cackling. They build upon some of the previous memorable bits (I admire Ryan for being so willing to relentlessly skewer his filmography) without it going stale. Luckily, they didn't ruin the movie by putting all the funny bits in the trailer and there's plenty to enjoy. They weren't afraid to push the envelope with the humour and stick around past the finale for a truly epic bit that rolls during the credits.,We've got all your favourite players back from Deadpool and I want to credit the writers for giving the opportunity for the characters to grow. Wade Wilson/Deadpool (Ryan Reynolds) is his typical wise-cracking self but he's found happiness and the change in his relationship with Vanessa (Morena Baccarin) has really impacted his outlook. Colossus (Stefan Kapicic) and Negasonic Teenage Warhead (Brianna Hildebrand) return as X-Men members and while they're familiar, they're attitude does change towards Deadpool's antics. We also get back Dopinder (Karan Soni), Weasel (T.J. Miller) and Blind Al (Leslie Uggams) as the rest of Wade's crew and they all bring something new or hilarious to the film. I also really liked the new members of the cast; Cable (Josh Brolin) and Domino (Zazie Beetz) lived up to the hype. Cable has more depth than I expected and a highlight of Deadpool 2 was how they worked in Domino's ability into the action. They were so creative in how her abilities changed the way the action flowed. I also really liked the character of Russell (Julian Dennison) and how they used him in the movie. He becomes the driving force of the plot and they shape his character in a way that finds a nice balance between too soft and too arrogant. If I had any complaints, Eddie Marsan's character is one of the weakest villains I've seen in the genre, he has little dimension and his screen time is limited. I also preferred the treatment of a few of the original characters in the initial installment (Negasonic and Blind Al were stronger in the first movie). Keep your eyes open for a couple of surprise appearances by a few famous faces in the X-Men lore. ,Director David Leitch is well known for his work in stunts and co-ordinating memorable action set pieces. This has carried over into his directorial efforts, I had a general knowledge of this going in, so I was expecting big things. Again, this movie rose to the task. It equals the original and, in some ways, surpasses it. I've already mentioned Domino, Colossus has a standout fight in the finale of the movie, the direction the movie decided to go with the initial X-Force mission was very surprising and I also liked Cable and Wade's first fight in the prison. The movie doesn't have a signature scene like the 12 bullets opening in Deadpool but there was never a moment that I was bored when the severed limbs were flying. Leitch and his team definitely got the job done and they balanced the humour and the thrills superbly. ,Ryan Reynolds followed up his signature turn as the foul-mouthed mercenary with an equally impressive performance. He's joined Hugh Jackman and Robert Downey Jr. as the benchmark of acting within the genre and I question if anyone else could even come close to him playing this character. Morena Baccarin still has great chemistry with Reynolds and their relationship remains as sweet as it did before. Brolin is appropriately cast as Cable, he lived up to high expectations and was appropriately gruff. Zazie Beetz is great as Domino, her casting was met with a lot of criticism but she's funny and I wish we had spent more time with her. Karan Soni carried over his charm from Deadpool as did Brianna Hildebrand. Julian Dennison did a solid job as Russell, I hadn't seen his previous work, but I can understand why so many people were excited to see him. Terry Crews, Bill Skarsgard, Lewis Tan and Rob Delaney were all good as the new X-Force members. Delaney was the standout as Peter. I also liked Stefan Kapicic and Leslie Uggams in their respective supporting parts.,One of the more surprisingly successful parts of the original Deadpool movie was the love story between Wade and Vanessa. It was surprisingly effective, and I dug the direction they took with it. Deadpool 2 is a story about family and while it still worked, it didn't tug at my heartstrings like the emotional core of the first one did. This isn't the fault of the actors or even the writers, it just didn't have the same impact.,In my opinion, this is a great sequel. It delivers a potent mix of both action and comedy. I was impressed that they refused to rest on the laurels of the original adventure and tried to do something new with it. If you found the first one too crude, too violent or too meta, I don't think this one will work for you. Its a movie that stands on its own, but you get the same creative expression as you did in the first one. I wouldn't say this is better than Deadpool, but it is on that same level for me. Its not perfect so I can't go all the way to a 10 but my actual rating is 9.5/10.\"\n",
      " \"Before the viewing of this film, I lowered my expectations knowing that majority of sequels are not as good as it's predecessors, but luckily the cast and crew made sure that this sequel is better in every aspect. Deadpool 2 abounds in funny moments, action scenes and importantly emotional ones, which were kind of weak in the first installment. This sequel bring some teachable moments on the table, but not too much, so the core of the movie is still very Deadpool-esque. The humor is similar to that of the first, with couple more pop culture jokes which might be a problem for some viewers, but for the most part they are easily understandable with the basic knowledge of pop culture. Along with well known original cast, we have couple of newcomers who fit in the cast perfectly. Brolin has a lot of screentime and he delivers in each scene. It features very entertaining references and cameos, and has one of the funniest post credits scenes in superhero movies, so be sure to stick around untill the very end. All in all, Deadpool 2 is a movie that can be enjoyable not only for fans of superhero genre, but for all who want to have a laugh while enjoying interesting action scenes.\"\n",
      " 'I have never written a movie review but I couldn\\'t let this jewel of a movie be slammed by the cynical critics without saying how much I loved it. This was the perfect movie for our family on Christmas Day. Don\\'t pay attention to the \"professional\" reviews; judge by the moviegoers who are praising the movie. It was moving, upbeat, and romantic. Can\\'t say enough good things about it. Hugh Jackman, Zac Efron, Michelle Williams, and Zendaya are all so talented. Go see it in the theater to really appreciate how magical this movie is.'\n",
      " \"I wasn't expecting it to be THAT good! It takes everything that was great about the first one and makes it even better!,David Leitch proves once again to be one of the best current directors in the genre of action, with really gory, fun fights. Also, the whole cast is great, but my favorite was Josh Brolin as Cable, I even liked him more here than in Infinity War.,If you liked the first one, please, go check this out!\"]\n"
     ]
    }
   ],
   "source": [
    "## input data\n",
    "data_path = \"Movie-crawler/IMDb/IMDb/spiders/reviews.csv\"\n",
    "origin_data = pd.read_csv(data_path)\n",
    "origin_data = origin_data[np.isfinite(origin_data['rating'])]\n",
    "origin_data = origin_data[pd.notnull(origin_data['review'])]\n",
    "print(origin_data.shape)\n",
    "corpus = origin_data.review.values\n",
    "print(corpus[:5])\n",
    "#y = np.array([0]*5331 + [1]*5331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1201fe940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1090b6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "origin_data.rating.value_counts().plot(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going', 'hard', 'pressed', 'find', 'movie', 'top', 'deadpool', 'david', 'leitch', 'takes', 'whole', 'new', 'level', 'entirely', 'works', 'namely', 'violence', 'creative', 'sure', 'defies', 'logic', 'time', 'laughing', 'riproaring', 'time', 'throughoutgo', 'movie', 'knowing', 'campy', 'corny', 'top', 'superheroactioncomedy', 'crazy', 'shooting', 'sequences', 'oneliners', 'youll', 'fine', 'go', 'movie', 'expecting', 'deep', 'plot', 'meaningful', 'conversations', 'among', 'characters', 'anything', 'remotely', 'resembling', 'serious', 'action', 'drama', 'movie', 'loved']\n",
      "going hard pressed find movie top deadpool david leitch takes whole new level entirely works namely violence creative sure defies logic time laughing riproaring time throughoutgo movie knowing campy corny top superheroactioncomedy crazy shooting sequences oneliners youll fine go movie expecting deep plot meaningful conversations among characters anything remotely resembling serious action drama movie loved\n",
      "Length of X: 12999\n"
     ]
    }
   ],
   "source": [
    "## Clean text\n",
    "\n",
    "def clean_doc(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    table = str.maketrans(\"\",\"\", string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "#    stemmer = SnowballStemmer('english')\n",
    "#    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "clean_corpus = []\n",
    "clean_words = []\n",
    "for i, s in enumerate(corpus):\n",
    "    clean_words.append(clean_doc(str(s)))\n",
    "    clean_corpus.append(\" \".join(clean_doc(s)))\n",
    "\n",
    "print(clean_words[0])\n",
    "print(clean_corpus[0])\n",
    "print(\"Length of X:\", len(clean_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(clean_corpus)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99, stratify=y)\n",
    "\n",
    "print(\"No. train-test samples:\", y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model\n",
    "\n",
    "- tfidf + NaiveBayers\n",
    "- tfidf + NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text vectorization\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, decode_error='ignore', ngram_range=(1,2), stop_words=\"english\")\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"After text vectorization,\\ntraining set:\", X_train_transformed.shape, \n",
    "      \"\\ntesting set:\", X_test_transformed.shape)\n",
    "\n",
    "## select 10% features based on chi2 stat\n",
    "def reduce_dim(X_train, X_test, y_train, percentile):\n",
    "    selector = SelectPercentile(chi2, percentile=percentile)\n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_reduced = selector.transform(X_train).toarray()\n",
    "    X_test_reduced = selector.transform(X_test).toarray()\n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "X_train_reduced, X_test_reduced = reduce_dim(X_train_transformed, \n",
    "                                             X_test_transformed, \n",
    "                                             y_train, percentile=10)\n",
    "\n",
    "print(\"After dimension reduction,\\ntraining set:\", X_train_reduced.shape, \n",
    "      \"\\ntesting set:\", X_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with Naive Bayes\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "print(\"Accuracy of train set: {:.2%}\".format(clf.score(X_train_reduced, y_train)))\n",
    "print(\"Accuracy of test set: {:.2%}\".format(clf.score(X_test_reduced, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with neural netword (1 hidden-layer)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "Xtrain = tokenizer.texts_to_matrix(X_train, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(X_test, mode='freq')\n",
    "ytrain = y_train\n",
    "ytest = y_test\n",
    "\n",
    "#print(Xtrain.shape, Xtest.shape)\n",
    "\n",
    "n_words = Xtrain.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(n_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(Xtrain, ytrain, epochs=20, batch_size=64, verbose=2)\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "\n",
    "- Train word embedding + PCA + visualization\n",
    "- embedding + LSTM\n",
    "- embedding + CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train embedding with word2vec\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "\n",
    "model = Word2Vec(clean_words, size=200, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "final_embeddings = model[model.wv.vocab]\n",
    "\n",
    "similarity = 1 - sp.distance.cdist(final_embeddings, final_embeddings, 'cosine')\n",
    "random_list = random.sample(range(len(words)), 10)\n",
    "for i in random_list:\n",
    "    top_sim = (-similarity[i,:]).argsort()[1:11]\n",
    "    print(\"nearest neighbors of {0}: \".format(words[i]), end=\"\")\n",
    "    for k in top_sim:\n",
    "        print(words[k], end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels):\n",
    "  plt.figure(figsize=(12, 12))\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "n_plot = 500\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(final_embeddings[:n_plot, :])\n",
    "\n",
    "labels = [words[i] for i in range(n_plot)]\n",
    "plot_with_labels(result, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build vocabulary\n",
    "\n",
    "def count_data(all_words, n_words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    index = dictionary.get(word, 0)\n",
    "    if index == 0:\n",
    "      unk_count += 1\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return count, dictionary, reversed_dictionary\n",
    "\n",
    "## words are indexed by overall frequency in the dataset, \n",
    "## so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "def build_data(sep_words):\n",
    "  data = list()\n",
    "  for lst in clean_words:\n",
    "    sent = list()\n",
    "    for w in lst:\n",
    "      if w in dictionary.keys():\n",
    "        sent.append(dictionary[w])\n",
    "      else:\n",
    "        sent.append(dictionary[\"UNK\"])\n",
    "    data.append(sent)\n",
    "  return data\n",
    "    \n",
    "count, dictionary, reverse_dictionary = build_dataset(all_words, vocab_size)\n",
    "data = build_data(clean_words)\n",
    "all_words = np.concatenate(clean_words)\n",
    "vocab_size = 10000\n",
    "\n",
    "print('Most common words (+UNK)', count[:10])\n",
    "print(\"5 sample sentences:\")\n",
    "for i in range(5):\n",
    "  if w in dictionary.keys():\n",
    "    print([reverse_dictionary[j] for j in data[i]], data[i], sep=\"\\n\")\n",
    "  else:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## embedding + LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(7)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=99, stratify=y)\n",
    "\n",
    "vector_length = 100\n",
    "sentence_length = 24\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=sentence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=sentence_length)\n",
    "\n",
    "#print(X_train.shape, X_test.shape)\n",
    "#print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=vector_length, input_length=sentence_length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding + LSTM + CNN\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=vector_length, input_length=sentence_length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## long reviews from IMDb \n",
    "## load directly from keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "vocab_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "sentence_length = 500\n",
    "embed_vecor_length = 32\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=sentence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=sentence_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_vecor_length, input_length=sentence_length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import data\n",
    "origin_data = pd.read_csv(\"data/kaggle.tsv\", sep='\\t')\n",
    "\n",
    "X = origin_data[\"Phrase\"].values\n",
    "y = origin_data[\"Sentiment\"].values\n",
    "\n",
    "origin_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(text, rm_stopwords=True):\n",
    "    \"\"\"\n",
    "    - Tokenize the words.\n",
    "    - Remove all punctuation from words.\n",
    "    - Remove all words that are not purely comprised of alphabetical characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    text=text.lower()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    table = str.maketrans(\"\",\"\", string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "clean_corpus = []\n",
    "clean_words = []\n",
    "for s in X:\n",
    "    clean_words.append(clean_sentence(s))\n",
    "    clean_corpus.append(\" \".join(clean_sentence(s)))\n",
    "\n",
    "print(clean_words[:10])\n",
    "print(clean_corpus[:10])\n",
    "print(\"Length of X:\", len(clean_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = np.concatenate(clean_words)\n",
    "count, dictionary, reverse_dictionary = build_dataset(all_words, vocab_size)\n",
    "data = build_data(clean_words)\n",
    "vocab_size = 10000\n",
    "\n",
    "print('Most common words (+UNK)', count[:10])\n",
    "print(\"5 sample sentences:\")\n",
    "for i in range(5):\n",
    "  if w in dictionary.keys():\n",
    "    print([reverse_dictionary[j] for j in data[i]], data[i], sep=\"\\n\")\n",
    "  else:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=99, stratify=y)\n",
    "\n",
    "sent_lens = [len(s) for s in clean_words]\n",
    "sentence_length = np.round((np.mean(sent_lens) + 2*np.std(sent_lens))).astype(int)\n",
    "vector_length = 100\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=sentence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=sentence_length)\n",
    "\n",
    "print(\"Shape of train-test data:\", X_train.shape, X_test.shape)\n",
    "\n",
    "Y_train = keras.utils.to_categorical(y_train, 5)\n",
    "Y_test   = keras.utils.to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=vector_length, input_length=sentence_length))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3, batch_size=32)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An alternative way to train a word2vec with tensorflow\n",
    "\n",
    "vocabulary_size = 1000\n",
    "def build_dataset(words, n_words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    index = dictionary.get(word, 0)\n",
    "    if index == 0:\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "    if data_index == len(data):\n",
    "      buffer.extend(data[0:span])\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "print(\"shape of batch:\", batch.shape, \"shape of labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 512  # Dimension of the embedding vector.\n",
    "skip_window = 4  # How many words to consider left and right.\n",
    "num_skips = 4  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample, used to adjust the weights.\n",
    "\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  with tf.device('/cpu:0'):\n",
    "    with tf.name_scope('embeddings'):\n",
    "      embeddings = tf.Variable(\n",
    "          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "      embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    with tf.name_scope('weights'):\n",
    "      nce_weights = tf.Variable(\n",
    "          tf.truncated_normal(\n",
    "              [vocabulary_size, embedding_size],\n",
    "              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "      nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=vocabulary_size))\n",
    "\n",
    "  with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.1).minimize(loss)\n",
    "\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                     valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training.\n",
    "num_steps = 80001\n",
    "with tf.Session(graph=graph) as session:\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    run_metadata = tf.RunMetadata()\n",
    "\n",
    "    _, loss_val = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict=feed_dict,\n",
    "        run_metadata=run_metadata)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "    \n",
    "    if step % 20000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 \n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels):\n",
    "  plt.figure(figsize=(16, 16))\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=1000, method='exact')\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec():\n",
    "    vec = []\n",
    "    for doc in corpus:\n",
    "        vec_sum = np.zeros([embedding_size])\n",
    "        for word in doc:\n",
    "            if word not in dictionary:\n",
    "                word = \"UNK\"\n",
    "            vec_sum += final_embeddings[dictionary[word]]\n",
    "        vec.append(vec_sum)\n",
    "    return np.array(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = doc2vec()\n",
    "y = np.array([0]*5331 + [1]*5331)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
